{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7519c3ed-9c92-4d27-95ec-f09441b2f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3329c37-d0e8-43d8-8300-ee30a5c13e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\\data.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Lines: 52644it [00:02, 21486.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded into DataFrame. Shape: (2380730, 2)\n",
      "   user   item\n",
      "0     0  28261\n",
      "1     0    388\n",
      "2     0   5731\n",
      "3     0    401\n",
      "4     0  28284\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join('Dataset', 'data.txt')\n",
    "\n",
    "raw_data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    # Estimate lines for progress bar (optional, but nice)\n",
    "    for line in tqdm(f, desc=\"Parsing Lines\"):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2: continue\n",
    "        \n",
    "        user_id = int(parts[0])\n",
    "        # Create a row for every item\n",
    "        for item_id in parts[1:]:\n",
    "            raw_data.append((user_id, int(item_id)))\n",
    "\n",
    "# 2. Convert to DataFrame\n",
    "df = pd.DataFrame(raw_data, columns=['user', 'item'])\n",
    "\n",
    "print(f\" Loaded into DataFrame. Shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f12c17a-df47-489c-ae27-09d6a020a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-Core Filtering (min=5)...\n",
      "Final Data: 2,372,615 interactions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Iterative K-Core Filtering\n",
    "\n",
    "# Define your threshold (e.g., 5 or 10)\n",
    "MIN_INTERACTIONS = 5 \n",
    "\n",
    "print(f\"Starting K-Core Filtering (min={MIN_INTERACTIONS})...\")\n",
    "\n",
    "# Loop until convergence (no more users/items removed)\n",
    "while True:\n",
    "    start_len = len(df)\n",
    "\n",
    "    # 1. Filter Users\n",
    "    u_counts = df['user'].value_counts()\n",
    "    df = df[df['user'].isin(u_counts[u_counts >= MIN_INTERACTIONS].index)]\n",
    "\n",
    "    # 2. Filter Items\n",
    "    i_counts = df['item'].value_counts()\n",
    "    df = df[df['item'].isin(i_counts[i_counts >= MIN_INTERACTIONS].index)]\n",
    "\n",
    "    # Stop if nothing changed\n",
    "    if len(df) == start_len:\n",
    "        break\n",
    "\n",
    "print(f\"Final Data: {len(df):,} interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1316bff2-6195-450c-b5f4-162c64556b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Dimensions: 52,642 Users x 88,416 Items\n",
      "Sparse Matrix created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Map IDs and Create Sparse Matrix\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# 1. Map to 0..N Indices\n",
    "# Create new columns for the matrix indices\n",
    "df['user_idx'] = df['user'].astype('category').cat.codes\n",
    "df['item_idx'] = df['item'].astype('category').cat.codes\n",
    "\n",
    "# 2. Create Lookup Dictionaries (To translate back later)\n",
    "user_map = dict(zip(df['user_idx'], df['user']))\n",
    "item_map = dict(zip(df['item_idx'], df['item']))\n",
    "\n",
    "# 3. Create the Sparse Matrix (CSR Format)\n",
    "n_users = df['user_idx'].max() + 1\n",
    "n_items = df['item_idx'].max() + 1\n",
    "\n",
    "print(f\"Matrix Dimensions: {n_users:,} Users x {n_items:,} Items\")\n",
    "\n",
    "rows = df['user_idx'].values\n",
    "cols = df['item_idx'].values\n",
    "data = np.ones(len(df)) # Implicit feedback (1 = interacted)\n",
    "\n",
    "matrix = sp.csr_matrix((data, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "print(\"Sparse Matrix created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050952b9-b201-41c4-ae83-483ae34b6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data 80/20...\n",
      "Train Interactions: 1,898,587\n",
      "Test Interactions:  474,028\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Split Data 80/20 (Masking)\n",
    "\n",
    "print(\"Splitting Data 80/20...\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Copy matrix to create Train and Test sets\n",
    "train_matrix = matrix.copy()\n",
    "test_matrix = matrix.copy()\n",
    "\n",
    "# Generate a random mask: True = Test (20%), False = Train (80%)\n",
    "mask = np.random.rand(len(matrix.data)) < 0.2\n",
    "\n",
    "# Create Train: Zero out the Test items (apply mask where True)\n",
    "train_matrix.data[mask] = 0\n",
    "train_matrix.eliminate_zeros()\n",
    "\n",
    "# Create Test: Zero out the Train items (apply mask where False)\n",
    "test_matrix.data[~mask] = 0\n",
    "test_matrix.eliminate_zeros()\n",
    "\n",
    "print(f\"Train Interactions: {train_matrix.nnz:,}\")\n",
    "print(f\"Test Interactions:  {test_matrix.nnz:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5798101c-04e7-4a99-a641-2e16d189a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to disk...\n",
      " Successfully saved:\n",
      " - train_matrix.npz\n",
      " - test_matrix.npz\n",
      " - user_map.pkl\n",
      " - item_map.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save Matrices and Maps\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "\n",
    "print(\"Saving data to disk...\")\n",
    "\n",
    "# 1. Save Matrices (Compressed NPZ format)\n",
    "sp.save_npz('train_matrix.npz', train_matrix)\n",
    "sp.save_npz('test_matrix.npz', test_matrix)\n",
    "\n",
    "# 2. Save Mappings (Pickle format)\n",
    "with open('user_map.pkl', 'wb') as f:\n",
    "    pickle.dump(user_map, f)\n",
    "    \n",
    "with open('item_map.pkl', 'wb') as f:\n",
    "    pickle.dump(item_map, f)\n",
    "\n",
    "print(\" Successfully saved:\")\n",
    "print(\" - train_matrix.npz\")\n",
    "print(\" - test_matrix.npz\")\n",
    "print(\" - user_map.pkl\")\n",
    "print(\" - item_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cc392a-b362-4186-9ddc-f394915ef61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk...\n",
      "✅ Loaded Train Matrix: (52642, 88416)\n",
      "✅ Loaded Test Matrix:  (52642, 88416)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Load Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading data from disk...\")\n",
    "\n",
    "# 1. Load Matrices\n",
    "train_matrix = sp.load_npz('train_matrix.npz')\n",
    "test_matrix = sp.load_npz('test_matrix.npz')\n",
    "\n",
    "# 2. Load Maps\n",
    "with open('user_map.pkl', 'rb') as f:\n",
    "    user_map = pickle.load(f)\n",
    "    \n",
    "with open('item_map.pkl', 'rb') as f:\n",
    "    item_map = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded Train Matrix: {train_matrix.shape}\")\n",
    "print(f\"Loaded Test Matrix:  {test_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0fc7f-fe02-43bd-9760-aa0a64f2c767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
